{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "import time\n",
    "import uuid\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "WebDriverException",
     "evalue": "Message: chrome not reachable\n  (Session info: chrome=100.0.4896.60)\nStacktrace:\n0   chromedriver                        0x0000000100eb7370 chromedriver + 4420464\n1   chromedriver                        0x0000000100e4e940 chromedriver + 3991872\n2   chromedriver                        0x0000000100aae418 chromedriver + 189464\n3   chromedriver                        0x0000000100a9e688 chromedriver + 124552\n4   chromedriver                        0x0000000100a9ef04 chromedriver + 126724\n5   chromedriver                        0x0000000100a99e98 chromedriver + 106136\n6   chromedriver                        0x0000000100b054f8 chromedriver + 546040\n7   chromedriver                        0x0000000100ad2248 chromedriver + 336456\n8   chromedriver                        0x0000000100e7b548 chromedriver + 4175176\n9   chromedriver                        0x0000000100e9195c chromedriver + 4266332\n10  chromedriver                        0x0000000100e96660 chromedriver + 4286048\n11  chromedriver                        0x0000000100e92124 chromedriver + 4268324\n12  chromedriver                        0x0000000100e7153c chromedriver + 4134204\n13  chromedriver                        0x0000000100ea9eb0 chromedriver + 4366000\n14  chromedriver                        0x0000000100eaa014 chromedriver + 4366356\n15  chromedriver                        0x0000000100ebda6c chromedriver + 4446828\n16  libsystem_pthread.dylib             0x00000001a02d54ec _pthread_start + 148\n17  libsystem_pthread.dylib             0x00000001a02d02d0 thread_start + 8\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mWebDriverException\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m/Users/serenawong/Desktop/data-collection-pipeline/scrapper_class.ipynb Cell 2'\u001b[0m in \u001b[0;36m<cell line: 230>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/serenawong/Desktop/data-collection-pipeline/scrapper_class.ipynb#ch0000001?line=218'>219</a>\u001b[0m             urllib\u001b[39m.\u001b[39mrequest\u001b[39m.\u001b[39murlretrieve(url_list[i], \u001b[39m\"\u001b[39m\u001b[39m./raw_data/images/\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m+\u001b[39mfolder_name\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m+\u001b[39m\u001b[39mstr\u001b[39m(i)\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.jpg\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/serenawong/Desktop/data-collection-pipeline/scrapper_class.ipynb#ch0000001?line=229'>230</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/serenawong/Desktop/data-collection-pipeline/scrapper_class.ipynb#ch0000001?line=230'>231</a>\u001b[0m     Scrapper()\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/serenawong/Desktop/data-collection-pipeline/scrapper_class.ipynb#ch0000001?line=231'>232</a>\u001b[0m     \u001b[39m# Scrapper.nav_by_search(Scrapper, \"top\")\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/serenawong/Desktop/data-collection-pipeline/scrapper_class.ipynb#ch0000001?line=232'>233</a>\u001b[0m     \u001b[39m# Scrapper.nav_by_url(Scrapper, Scrapper.header_url_list(Scrapper)[1])\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/serenawong/Desktop/data-collection-pipeline/scrapper_class.ipynb#ch0000001?line=233'>234</a>\u001b[0m     \u001b[39m# Scrapper.scroll_to_bottom(Scrapper)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/serenawong/Desktop/data-collection-pipeline/scrapper_class.ipynb#ch0000001?line=236'>237</a>\u001b[0m     \u001b[39m# Scrapper.switch_tab(Scrapper, 0)\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/serenawong/Desktop/data-collection-pipeline/scrapper_class.ipynb#ch0000001?line=237'>238</a>\u001b[0m     \u001b[39m# Scrapper.close_tab(Scrapper)\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/serenawong/Desktop/data-collection-pipeline/scrapper_class.ipynb#ch0000001?line=238'>239</a>\u001b[0m driver \u001b[39m=\u001b[39m webdriver\u001b[39m.\u001b[39mChrome(service \u001b[39m=\u001b[39m Service(\u001b[39m'\u001b[39m\u001b[39m./chromedriver\u001b[39m\u001b[39m'\u001b[39m))\n",
      "\u001b[1;32m/Users/serenawong/Desktop/data-collection-pipeline/scrapper_class.ipynb Cell 2'\u001b[0m in \u001b[0;36mScrapper.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/serenawong/Desktop/data-collection-pipeline/scrapper_class.ipynb#ch0000001?line=3'>4</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/serenawong/Desktop/data-collection-pipeline/scrapper_class.ipynb#ch0000001?line=5'>6</a>\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/serenawong/Desktop/data-collection-pipeline/scrapper_class.ipynb#ch0000001?line=6'>7</a>\u001b[0m         driver\u001b[39m.\u001b[39;49mfind_element(by\u001b[39m=\u001b[39;49mBy\u001b[39m.\u001b[39;49mCSS_SELECTOR, value\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mbutton[class=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39msc-kEqXSa sc-iqAclL sc-ciSkZP hQtFsL cmWQHQ exduyW\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m]\u001b[39;49m\u001b[39m\"\u001b[39;49m)\u001b[39m.\u001b[39mclick()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/serenawong/Desktop/data-collection-pipeline/scrapper_class.ipynb#ch0000001?line=7'>8</a>\u001b[0m     \u001b[39mexcept\u001b[39;00m NoSuchElementException:\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/serenawong/Desktop/data-collection-pipeline/scrapper_class.ipynb#ch0000001?line=8'>9</a>\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mno (accept cookies button) found\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniforge3/envs/data-collection-pipeline-env/lib/python3.9/site-packages/selenium/webdriver/remote/webdriver.py:1248\u001b[0m, in \u001b[0;36mWebDriver.find_element\u001b[0;34m(self, by, value)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/serenawong/miniforge3/envs/data-collection-pipeline-env/lib/python3.9/site-packages/selenium/webdriver/remote/webdriver.py?line=1244'>1245</a>\u001b[0m     by \u001b[39m=\u001b[39m By\u001b[39m.\u001b[39mCSS_SELECTOR\n\u001b[1;32m   <a href='file:///Users/serenawong/miniforge3/envs/data-collection-pipeline-env/lib/python3.9/site-packages/selenium/webdriver/remote/webdriver.py?line=1245'>1246</a>\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m[name=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m]\u001b[39m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m value\n\u001b[0;32m-> <a href='file:///Users/serenawong/miniforge3/envs/data-collection-pipeline-env/lib/python3.9/site-packages/selenium/webdriver/remote/webdriver.py?line=1247'>1248</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexecute(Command\u001b[39m.\u001b[39;49mFIND_ELEMENT, {\n\u001b[1;32m   <a href='file:///Users/serenawong/miniforge3/envs/data-collection-pipeline-env/lib/python3.9/site-packages/selenium/webdriver/remote/webdriver.py?line=1248'>1249</a>\u001b[0m     \u001b[39m'\u001b[39;49m\u001b[39musing\u001b[39;49m\u001b[39m'\u001b[39;49m: by,\n\u001b[1;32m   <a href='file:///Users/serenawong/miniforge3/envs/data-collection-pipeline-env/lib/python3.9/site-packages/selenium/webdriver/remote/webdriver.py?line=1249'>1250</a>\u001b[0m     \u001b[39m'\u001b[39;49m\u001b[39mvalue\u001b[39;49m\u001b[39m'\u001b[39;49m: value})[\u001b[39m'\u001b[39m\u001b[39mvalue\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[0;32m~/miniforge3/envs/data-collection-pipeline-env/lib/python3.9/site-packages/selenium/webdriver/remote/webdriver.py:425\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[0;34m(self, driver_command, params)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/serenawong/miniforge3/envs/data-collection-pipeline-env/lib/python3.9/site-packages/selenium/webdriver/remote/webdriver.py?line=422'>423</a>\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_executor\u001b[39m.\u001b[39mexecute(driver_command, params)\n\u001b[1;32m    <a href='file:///Users/serenawong/miniforge3/envs/data-collection-pipeline-env/lib/python3.9/site-packages/selenium/webdriver/remote/webdriver.py?line=423'>424</a>\u001b[0m \u001b[39mif\u001b[39;00m response:\n\u001b[0;32m--> <a href='file:///Users/serenawong/miniforge3/envs/data-collection-pipeline-env/lib/python3.9/site-packages/selenium/webdriver/remote/webdriver.py?line=424'>425</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49merror_handler\u001b[39m.\u001b[39;49mcheck_response(response)\n\u001b[1;32m    <a href='file:///Users/serenawong/miniforge3/envs/data-collection-pipeline-env/lib/python3.9/site-packages/selenium/webdriver/remote/webdriver.py?line=425'>426</a>\u001b[0m     response[\u001b[39m'\u001b[39m\u001b[39mvalue\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_unwrap_value(\n\u001b[1;32m    <a href='file:///Users/serenawong/miniforge3/envs/data-collection-pipeline-env/lib/python3.9/site-packages/selenium/webdriver/remote/webdriver.py?line=426'>427</a>\u001b[0m         response\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mvalue\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[1;32m    <a href='file:///Users/serenawong/miniforge3/envs/data-collection-pipeline-env/lib/python3.9/site-packages/selenium/webdriver/remote/webdriver.py?line=427'>428</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/miniforge3/envs/data-collection-pipeline-env/lib/python3.9/site-packages/selenium/webdriver/remote/errorhandler.py:247\u001b[0m, in \u001b[0;36mErrorHandler.check_response\u001b[0;34m(self, response)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/serenawong/miniforge3/envs/data-collection-pipeline-env/lib/python3.9/site-packages/selenium/webdriver/remote/errorhandler.py?line=244'>245</a>\u001b[0m         alert_text \u001b[39m=\u001b[39m value[\u001b[39m'\u001b[39m\u001b[39malert\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    <a href='file:///Users/serenawong/miniforge3/envs/data-collection-pipeline-env/lib/python3.9/site-packages/selenium/webdriver/remote/errorhandler.py?line=245'>246</a>\u001b[0m     \u001b[39mraise\u001b[39;00m exception_class(message, screen, stacktrace, alert_text)  \u001b[39m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[39;00m\n\u001b[0;32m--> <a href='file:///Users/serenawong/miniforge3/envs/data-collection-pipeline-env/lib/python3.9/site-packages/selenium/webdriver/remote/errorhandler.py?line=246'>247</a>\u001b[0m \u001b[39mraise\u001b[39;00m exception_class(message, screen, stacktrace)\n",
      "\u001b[0;31mWebDriverException\u001b[0m: Message: chrome not reachable\n  (Session info: chrome=100.0.4896.60)\nStacktrace:\n0   chromedriver                        0x0000000100eb7370 chromedriver + 4420464\n1   chromedriver                        0x0000000100e4e940 chromedriver + 3991872\n2   chromedriver                        0x0000000100aae418 chromedriver + 189464\n3   chromedriver                        0x0000000100a9e688 chromedriver + 124552\n4   chromedriver                        0x0000000100a9ef04 chromedriver + 126724\n5   chromedriver                        0x0000000100a99e98 chromedriver + 106136\n6   chromedriver                        0x0000000100b054f8 chromedriver + 546040\n7   chromedriver                        0x0000000100ad2248 chromedriver + 336456\n8   chromedriver                        0x0000000100e7b548 chromedriver + 4175176\n9   chromedriver                        0x0000000100e9195c chromedriver + 4266332\n10  chromedriver                        0x0000000100e96660 chromedriver + 4286048\n11  chromedriver                        0x0000000100e92124 chromedriver + 4268324\n12  chromedriver                        0x0000000100e7153c chromedriver + 4134204\n13  chromedriver                        0x0000000100ea9eb0 chromedriver + 4366000\n14  chromedriver                        0x0000000100eaa014 chromedriver + 4366356\n15  chromedriver                        0x0000000100ebda6c chromedriver + 4446828\n16  libsystem_pthread.dylib             0x00000001a02d54ec _pthread_start + 148\n17  libsystem_pthread.dylib             0x00000001a02d02d0 thread_start + 8\n"
     ]
    }
   ],
   "source": [
    "driver = webdriver.Chrome(service = Service('./chromedriver'))\n",
    "driver.get(\"https://www.depop.com\")\n",
    "    \n",
    "class Scrapper:\n",
    "\n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "        try:\n",
    "            driver.find_element(by=By.CSS_SELECTOR, value=\"button[class='sc-kEqXSa sc-iqAclL sc-ciSkZP hQtFsL cmWQHQ exduyW']\").click()\n",
    "        except NoSuchElementException:\n",
    "            print(\"no (accept cookies button) found\")\n",
    "\n",
    "    def nav_by_search(self, search_item):\n",
    "        search_url = \"https://www.depop.com/search/?q=\"+search_item\n",
    "        driver.get(search_url)\n",
    "\n",
    "    def nav_by_shop(self, shop_name):\n",
    "        shop_url = \"https://www.depop.com/\"+shop_name\n",
    "        driver.get(shop_url)\n",
    "\n",
    "    def header_url_list(self):\n",
    "        header_url = []\n",
    "        top_level_elements = driver.find_elements(by=By.CLASS_NAME, value=\"styles__NavigationItem-sc-__sc-10mkzda-3\")\n",
    "        for i in top_level_elements:\n",
    "            try:\n",
    "                child_level_elements = i.find_elements(by=By.XPATH, value=\".//div/ul/li\")\n",
    "                for j in child_level_elements:\n",
    "                    child_nav_option = j.find_element(by=By.XPATH, value=\".//a\").get_attribute(\"href\")\n",
    "                    header_url.append(child_nav_option)\n",
    "            except NoSuchElementException:\n",
    "                pass\n",
    "        return header_url\n",
    "\n",
    "\n",
    "    def listing_url(self, listing_no):\n",
    "        listing = driver.find_elements(by=By.CLASS_NAME, value=\"styles__ProductCardContainer-sc-__sc-13q41bc-8\")\n",
    "        listing_url = listing[listing_no].find_element(by=By.XPATH, value=\".//a\").get_attribute(\"href\")\n",
    "        return listing_url\n",
    "\n",
    "    def scroll_to_bottom(self):\n",
    "        webpage_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        current_height = 0\n",
    "        while current_height <= webpage_height:\n",
    "            driver.execute_script(\"window.scrollTo(0,\"+str(current_height)+\")\")\n",
    "            current_height += 30\n",
    "\n",
    "    def back_page(self):\n",
    "        driver.execute_script(\"window.history.go(-1)\")\n",
    "\n",
    "    def open_url_new_tab(self, url):\n",
    "        driver.execute_script(\"window.open('\" + url + \"');\")\n",
    "\n",
    "    def close_tab(self):\n",
    "        driver.close()\n",
    "\n",
    "    def switch_tab(self, tab_no):\n",
    "        driver.switch_to.window(driver.window_handles[tab_no])\n",
    "\n",
    "    def get_shop_data(self):\n",
    "        data_dictionary={}\n",
    "\n",
    "        username = driver.find_element(by=By.CLASS_NAME, value=\"styles__UserName-sc-__r941b9-4\").get_attribute(\"innerText\")\n",
    "        data_dictionary.update({\"Username\": username})\n",
    "        items_sold = driver.find_element(by=By.XPATH, value=\"//*[@id='main']/div[1]/div[1]/div/div[2]/div[1]/p\").get_attribute(\"innerText\")\n",
    "        data_dictionary.update({\"Items Sold\": items_sold})\n",
    "        last_activity = driver.find_element(by=By.XPATH, value='//*[@id=\"main\"]/div[1]/div[1]/div/div[2]/div[2]/p').get_attribute(\"innerText\")\n",
    "        data_dictionary.update({\"Last Activity\": last_activity})\n",
    "        followers = driver.find_element(by=By.XPATH, value='//*[@id=\"main\"]/div[1]/div[2]/button[1]/p[1]').get_attribute(\"innerText\")\n",
    "        data_dictionary.update({\"Followers\": followers})\n",
    "        following = driver.find_element(by=By.XPATH, value='//*[@id=\"main\"]/div[1]/div[2]/button[2]/p[1]').get_attribute(\"innerText\")\n",
    "        data_dictionary.update({\"Following\": following})\n",
    "        try:\n",
    "            bio_text = driver.find_element(by=By.XPATH, value='//*[@id=\"main\"]/div[1]/div[3]/p').get_attribute(\"innerText\")\n",
    "        except NoSuchElementException:\n",
    "            bio_text = \"None\"\n",
    "        data_dictionary.update({\"Bio Description\":bio_text})\n",
    "        return data_dictionary        \n",
    "\n",
    "    def product_availability(self):\n",
    "        try:\n",
    "            driver.find_element(by=By.CSS_SELECTOR, value=\"button.egHolT[color='yellow']\")\n",
    "            sold = True\n",
    "        except NoSuchElementException:\n",
    "            sold = False\n",
    "        print(\"Product sold? \"+ str(sold))\n",
    "        return sold\n",
    "\n",
    "\n",
    "    def get_product_page_data(self):\n",
    "\n",
    "        data_dictionary = {}\n",
    "\n",
    "        data_dictionary.update({\"Product ID\": driver.current_url})\n",
    "        data_dictionary.update({\"UUID\": str(uuid.uuid4())})\n",
    "        shop_name = driver.find_element(by=By.CSS_SELECTOR, value=\"a[data-testid='bio__username']\").get_attribute(\"innerText\")\n",
    "        data_dictionary.update({\"Shop Name\": shop_name})\n",
    "        postcode = driver.find_element(by=By.CSS_SELECTOR, value=\"p[data-testid='bio__address']\").get_attribute(\"innerText\")\n",
    "        data_dictionary.update({\"Location\": postcode})\n",
    "\n",
    "        # <p data-testid=\"bio__address\" type=\"caption1\" class=\"sc-jrsJWt eoZhdh\">United Kingdom</p>\n",
    "\n",
    "        # rating = 0\n",
    "        # for i in range(1, 5):\n",
    "\n",
    "        #     star = driver.find_element(by=By.XPATH, value=\"//*[@id='feedback-star-\" + str(i) + \"-19262048']/title\").get_attribute(\"innerText\")\n",
    "        #     if star == \"Full Star\":\n",
    "        #         rating += 1\n",
    "        #     elif star == \"Half Star\":\n",
    "        #         rating += 0.5\n",
    "        #     elif star == \"Empty Star\":\n",
    "        #         rating += 0\n",
    "        \n",
    "        review_num = driver.find_element(by=By.CSS_SELECTOR, value=\"p[data-testid='feedback-btn__total']\").get_attribute(\"innerText\")\n",
    "        data_dictionary.update({\"No. of Reviews\": review_num})\n",
    "        sold = driver.find_element(by=By.CSS_SELECTOR, value=\"div[data-testid='signals__sold']\")\n",
    "        sold_items = sold.find_element(by=By.XPATH, value=\".//p\").get_attribute(\"innerText\")\n",
    "        # one_size = driver.find_element(by=By.CSS_SELECTOR, value=\"tr[data-testid='product__singleSize']\")\n",
    "        # size = one_size.find_element(by=By.XPATH, value=\".//td\").get_attribute(\"innerText\")\n",
    "        data_dictionary.update({\"No. of Items Sold\": sold_items})\n",
    "        active = driver.find_element(by=By.CSS_SELECTOR, value=\"div[data-testid='signals__active']\")\n",
    "        activity = active.find_element(by=By.XPATH, value=\".//p\").get_attribute(\"innerText\")\n",
    "        data_dictionary.update({\"Last Active Date\": activity})\n",
    "\n",
    "        try:\n",
    "            likes_num = driver.find_element(by=By.CSS_SELECTOR, value=\"span[data-testid='like-count']\").get_attribute(\"innerText\")\n",
    "        except NoSuchElementException:\n",
    "            likes_num = \"0\"\n",
    "        data_dictionary.update({\"No. of Likes\": likes_num})\n",
    "\n",
    "        try:\n",
    "            price = driver.find_element(by=By.CSS_SELECTOR, value=\"p[data-testid='discountedPrice']\").get_attribute(\"innerText\")\n",
    "            data_dictionary.update({\"Price\": price})\n",
    "            data_dictionary.update({\"Discount\": True})\n",
    "        except NoSuchElementException:\n",
    "            price = driver.find_element(by=By.CSS_SELECTOR, value=\"p[data-testid='fullPrice']\").get_attribute(\"innerText\")\n",
    "            data_dictionary.update({\"Price\": price})\n",
    "            data_dictionary.update({\"Discount\": True})\n",
    "\n",
    "        item_description = driver.find_element(by=By.CSS_SELECTOR, value=\"p[data-testid='product__description']\").get_attribute(\"innerText\")\n",
    "        data_dictionary.update({\"Item Description\": item_description})\n",
    "        last_refresh = driver.find_element(by=By.CSS_SELECTOR, value=\"time[data-testid='time']\").get_attribute(\"innerText\")\n",
    "        data_dictionary.update({\"Last Update\": last_refresh})\n",
    "\n",
    "        try:\n",
    "            one_size = driver.find_element(by=By.CSS_SELECTOR, value=\"tr[data-testid='product__singleSize']\")\n",
    "            size = one_size.find_element(by=By.XPATH, value=\".//td\").get_attribute(\"innerText\")\n",
    "\n",
    "        except NoSuchElementException:\n",
    "            size = \"Multiple sizes\"\n",
    "        data_dictionary.update({\"Sizes Available\": size})\n",
    "\n",
    "        try:\n",
    "            brand = driver.find_element(by=By.CSS_SELECTOR, value=\"a[data-testid='product__brand']\").get_attribute(\"innerText\")\n",
    "        except NoSuchElementException:\n",
    "            brand = \"None\"\n",
    "        data_dictionary.update({\"Brand\": brand})\n",
    "\n",
    "        condition = driver.find_element(by=By.CSS_SELECTOR, value=\"td[data-testid='product__condition']\").get_attribute(\"innerText\")\n",
    "        data_dictionary.update({\"Item Condition\": condition})\n",
    "\n",
    "        try:\n",
    "            colour = driver.find_element(by=By.CSS_SELECTOR, value=\"td[data-testid='product__colour']\").get_attribute(\"innerText\")\n",
    "        except NoSuchElementException:\n",
    "            colour = \"None\"\n",
    "        data_dictionary.update({\"Colour\": colour})\n",
    "\n",
    "        try:\n",
    "            style_tag = driver.find_element(by=By.CSS_SELECTOR, value=\"td[data-testid='selected__styles']\").get_attribute(\"innerText\")\n",
    "        except NoSuchElementException:\n",
    "            style_tag = \"None\"\n",
    "        data_dictionary.update({\"Style\": style_tag})\n",
    "\n",
    "        img_urls = []\n",
    "        image_elements = driver.find_elements(by=By.CSS_SELECTOR, value=\"img[class='LazyLoadImage__StyledImage-sc-__bquzot-1 doaiRN styles__LazyImage-sc-__sc-1fk4zep-9 hRpLaq']\")\n",
    "        for image_element in image_elements:\n",
    "            img_url = image_element.get_attribute(\"src\")\n",
    "            if img_url not in img_urls:\n",
    "                img_urls.append(img_url)\n",
    "        data_dictionary.update({\"Image Urls\": img_urls})\n",
    "\n",
    "        self.download_images(img_urls, driver.current_url)\n",
    "\n",
    "        # print(rating)\n",
    "\n",
    "        return data_dictionary\n",
    "\n",
    "    def scrape_listing(self, number_of_listing, dict_name):\n",
    "        for i in range(number_of_listing):\n",
    "            if ((i)%24 == 0) and i !=0:\n",
    "                self.scroll_to_bottom()\n",
    "                time.sleep(1)\n",
    "            listing_url = self.listing_url(i)\n",
    "            self.open_url_new_tab(listing_url)\n",
    "            self.switch_tab(1)\n",
    "            self.scroll_to_bottom()\n",
    "            scraped_data = self.get_product_page_data()\n",
    "            self.add_data(scraped_data, \"./raw_data/data.json\", dict_name)\n",
    "            self.close_tab()\n",
    "            self.switch_tab(0)\n",
    "\n",
    "    def create_json_file(self, filepath, main_dictionaries,):\n",
    "        output ={}\n",
    "        for dictionary in main_dictionaries:\n",
    "            output[dictionary]=[]\n",
    "        with open(filepath, \"w\") as outfile:\n",
    "            json.dump(output, outfile)\n",
    "\n",
    "    def add_data(self, new_data, filepath, main_dictionary):\n",
    "        with open(filepath, 'r+') as file:\n",
    "            file_data = json.load(file)\n",
    "            file_data[main_dictionary].append(new_data)\n",
    "            file.seek(0)\n",
    "            json.dump(file_data, file)\n",
    "\n",
    "    def download_images(self, url_list, product_id):\n",
    "        folder = product_id[22:]\n",
    "        folder_name = folder.replace('/', '-')\n",
    "        os.mkdir(\"./raw_data/images/\"+folder_name)\n",
    "        for i in range(len(url_list)):\n",
    "            open(\"./raw_data/images/\"+folder_name+\"/\"+str(i)+\".jpg\", 'w').close()\n",
    "            urllib.request.urlretrieve(url_list[i], \"./raw_data/images/\"+folder_name+\"/\"+str(i)+\".jpg\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    Scrapper()\n",
    "    # Scrapper.nav_by_search(Scrapper, \"top\")\n",
    "    # Scrapper.nav_by_url(Scrapper, Scrapper.header_url_list(Scrapper)[1])\n",
    "    # Scrapper.scroll_to_bottom(Scrapper)\n",
    "    # Scrapper.back_page(Scrapper)\n",
    "    # Scrapper.open_url_new_tab(Scrapper, Scrapper.nav_listing(Scrapper, 2))\n",
    "    # Scrapper.switch_tab(Scrapper, 0)\n",
    "    # Scrapper.close_tab(Scrapper)\n",
    "\n",
    "bot = Scrapper()\n",
    "bot.nav_by_shop(\"cleopatress\")\n",
    "print(\"Current Page Title is : %s\" %driver.title)\n",
    "bot.create_json_file(\"./raw_data/data.json\", [\"Test_ShopData\"])\n",
    "bot.scrape_listing(150, \"Test_ShopData\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "\n",
    "list = [\"https://media-photos.depop.com/b0/14667927/792417311_9d826f5f4382490cb28fdb1bb13a3db1/P0.jpg\", \"https://media-photos.depop.com/b0/14667927/792417332_6df7c572bc46451ba5adf066fc959b37/P0.jpg\"]\n",
    "for i in range(len(list)):\n",
    "    open(\"./raw_data/images/\"+\"test/\"+str(i)+\".jpg\", 'w').close()\n",
    "# fullfilename = os.path.join(\"images/test\", \"1.jpg\")\n",
    "    urllib.request.urlretrieve(list[i], \"./raw_data/images/\"+\"test/\"+str(i)+\".jpg\")\n",
    "\n",
    "# with open(\"./raw_data/images/\"+\"test\", )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir(\"raw_data/images/products/robinrebecca-y2k-top-white-open-front-7494/\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/serenawong/Desktop/data-collection-pipeline/scrapper_class.ipynb Cell 5'\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/serenawong/Desktop/data-collection-pipeline/scrapper_class.ipynb#ch0000003?line=7'>8</a>\u001b[0m     memo[n] \u001b[39m=\u001b[39m result\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/serenawong/Desktop/data-collection-pipeline/scrapper_class.ipynb#ch0000003?line=8'>9</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/serenawong/Desktop/data-collection-pipeline/scrapper_class.ipynb#ch0000003?line=9'>10</a>\u001b[0m \u001b[39mprint\u001b[39m(fib(\u001b[39m6\u001b[39;49m, []))\n",
      "\u001b[1;32m/Users/serenawong/Desktop/data-collection-pipeline/scrapper_class.ipynb Cell 5'\u001b[0m in \u001b[0;36mfib\u001b[0;34m(n, memo)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/serenawong/Desktop/data-collection-pipeline/scrapper_class.ipynb#ch0000003?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfib\u001b[39m(n, memo):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/serenawong/Desktop/data-collection-pipeline/scrapper_class.ipynb#ch0000003?line=1'>2</a>\u001b[0m     \u001b[39mif\u001b[39;00m memo[n] \u001b[39m!=\u001b[39m null:\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/serenawong/Desktop/data-collection-pipeline/scrapper_class.ipynb#ch0000003?line=2'>3</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m memo\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/serenawong/Desktop/data-collection-pipeline/scrapper_class.ipynb#ch0000003?line=3'>4</a>\u001b[0m     \u001b[39mif\u001b[39;00m n \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39mor\u001b[39;00m n \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m:\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "lis = []\n",
    "def fib(n, memo):\n",
    "    if memo[n] != null:\n",
    "        return memo\n",
    "    if n == 1 or n == 2:\n",
    "        result = 1\n",
    "    else:\n",
    "        result = fib(n-1)+fib(n-2)\n",
    "    memo[n] = result\n",
    "    return result\n",
    "print(fib(6, lis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "59a815aa6b510b7c00ae6aa3124b3da72856d47b254de79486eb33c0d94934ee"
  },
  "kernelspec": {
   "display_name": "Python 3.9.11 ('data-collection-pipeline-env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
