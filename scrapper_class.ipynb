{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "import time\n",
    "import uuid\n",
    "import json\n",
    "import os\n",
    "import urllib\n",
    "import urllib.request\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Scrapper:\n",
    "    \"\"\"\n",
    "    This class contains methods to scrape data from the website 'www.depop.com'.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Checks if 'accept cookies' button is present, and by passes it if present.\n",
    "        \"\"\"\n",
    "\n",
    "        self.driver = webdriver.Chrome(service = Service('./chromedriver'))\n",
    "        self.driver.get(\"https://www.depop.com\")\n",
    "        try:\n",
    "            self.driver.find_element(by=By.CSS_SELECTOR, value=\"button[class='sc-kEqXSa sc-iqAclL sc-ciSkZP hQtFsL cmWQHQ exduyW']\").click()\n",
    "        except NoSuchElementException:\n",
    "            print(\"no (accept cookies button) found\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def nav_by_search(self, search_item):\n",
    "        \"\"\"\n",
    "        Navigate to a specific search result corresponding to the 'search_item' input string.\n",
    "        \"\"\"\n",
    "\n",
    "        search_url = \"https://www.depop.com/search/?q=\"+search_item\n",
    "        self.driver.get(search_url)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def nav_by_shop(self, shop_name):\n",
    "        \"\"\"\n",
    "        Navigate to a specific store front corresponding to the 'shop_name' input string.\n",
    "        \"\"\"\n",
    "\n",
    "        shop_url = \"https://www.depop.com/\"+shop_name\n",
    "        self.driver.get(shop_url)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def header_url_list(self):\n",
    "        \"\"\"\n",
    "        Returns a list with the URL from the webpage header.\n",
    "        \"\"\"\n",
    "\n",
    "        header_url = []\n",
    "        top_level_elements = self.driver.find_elements(by=By.CSS_SELECTOR, value=\"li[data-testid='treeNavigation__dropdown']\")\n",
    "        for i in top_level_elements:\n",
    "            try:\n",
    "                child_level_elements = i.find_elements(by=By.XPATH, value=\".//div/ul/li\")\n",
    "                for j in child_level_elements:\n",
    "                    child_nav_option = j.find_element(by=By.XPATH, value=\".//a\").get_attribute(\"href\")\n",
    "                    header_url.append(child_nav_option)\n",
    "            except NoSuchElementException:\n",
    "                pass\n",
    "        return header_url\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def listing_url(self, n):\n",
    "        \"\"\"\n",
    "        Takes the input 'n' and returns the URL of the nth listing.\n",
    "        \"\"\"\n",
    "        listing = self.driver.find_elements(by=By.CLASS_NAME, value=\"styles__ProductCardContainer-sc-__sc-13q41bc-8\")\n",
    "        listing_url = listing[n].find_element(by=By.XPATH, value=\".//a\").get_attribute(\"href\")\n",
    "        return listing_url\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def scroll_to_bottom(self):\n",
    "        \"\"\"\n",
    "        Scrolls to the bottom of webpage.\n",
    "        \"\"\"\n",
    "\n",
    "        webpage_height = self.driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        current_height = 0\n",
    "        while current_height <= webpage_height:\n",
    "            self.driver.execute_script(\"window.scrollTo(0,\"+str(current_height)+\")\")\n",
    "            current_height += 30\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def back_page(self):\n",
    "        \"\"\"\n",
    "        Navigates to the previous webpage.\n",
    "        \"\"\"\n",
    "\n",
    "        self.driver.execute_script(\"window.history.go(-1)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def open_url_new_tab(self, url):\n",
    "        \"\"\"\n",
    "        Takes an input URL string 'url' and opens the URL in a new tab.\n",
    "        \"\"\"\n",
    "        self.driver.execute_script(\"window.open('\" + url + \"');\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def close_tab(self):\n",
    "        \"\"\"\n",
    "        Closes current active tab.\n",
    "        \"\"\"\n",
    "        self.driver.close()\n",
    "\n",
    "\n",
    "\n",
    "    def switch_tab(self, tab_no):\n",
    "        \"\"\"\n",
    "        Switch to tab with the index 'tab_no'.\n",
    "        \"\"\"\n",
    "        self.driver.switch_to.window(self.driver.window_handles[tab_no])\n",
    "\n",
    "    \n",
    "\n",
    "    def close_browser(self):\n",
    "        \"\"\"\n",
    "        Close current active browser\n",
    "        \"\"\"\n",
    "        self.driver.quit()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def get_shop_data(self):\n",
    "        \"\"\"\n",
    "        Returns data from a store front in the form of a dictionary.\n",
    "        \n",
    "        Data collected are: Username, Items sold, Last Active Date, Followers, Following, Bio description.\n",
    "        \"\"\"\n",
    "        data_dictionary={}\n",
    "\n",
    "        username = self.driver.find_element(by=By.CSS_SELECTOR, value=\"p[data-testid='username']\").get_attribute(\"innerText\")\n",
    "        data_dictionary.update({\"Username\": username})\n",
    "\n",
    "        items_sold_container = self.driver.find_element(by=By.CSS_SELECTOR, value=\"div[data-testid='signals__sold']\")\n",
    "        items_sold = items_sold_container.find_element(by=By.XPATH, value=\".//p\").get_attribute(\"innerText\")\n",
    "        data_dictionary.update({\"Items Sold\": items_sold})\n",
    "\n",
    "        last_activity_container = self.driver.find_element(by=By.CSS_SELECTOR, value=\"div[data-testid='signals__active']\")\n",
    "        last_activity = last_activity_container.find_element(by=By.XPATH, value=\".//p\").get_attribute(\"innerText\")\n",
    "        data_dictionary.update({\"Last Active Date\": last_activity})\n",
    "        \n",
    "        followers_container = self.driver.find_element(by=By.CSS_SELECTOR, value=\"button[aria-label='followers']\")\n",
    "        followers = followers_container.find_element(by=By.XPATH, value=\".//p\").get_attribute(\"innerText\")\n",
    "        data_dictionary.update({\"Followers\": followers})\n",
    "\n",
    "        following_container = self.driver.find_element(by=By.CSS_SELECTOR, value=\"button[aria-label='following']\")\n",
    "        following = following_container.find_element(by=By.XPATH, value=\".//p\").get_attribute(\"innerText\")\n",
    "        data_dictionary.update({\"Following\": following})\n",
    "\n",
    "        try:\n",
    "            bio_text = self.driver.find_element(by=By.CLASS_NAME, value='styles__UserDescription-sc-__r941b9-7').get_attribute(\"innerText\")\n",
    "        except NoSuchElementException:\n",
    "            bio_text = \"None\"\n",
    "        data_dictionary.update({\"Bio Description\":bio_text})\n",
    "        return data_dictionary        \n",
    "\n",
    "    def product_availability(self):\n",
    "        \"\"\"\n",
    "        Checks if an item is sold.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.driver.find_element(by=By.CSS_SELECTOR, value=\"button.egHolT[color='yellow']\")\n",
    "            sold = True\n",
    "        except NoSuchElementException:\n",
    "            sold = False\n",
    "        print(\"Product sold? \"+ str(sold))\n",
    "        return sold\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def get_product_page_data(self, img_test_folder_name):\n",
    "        \"\"\"\n",
    "        Returns data from a item listing in the form of a dictionary, and generates a UUID to identify data point.\n",
    "        \n",
    "        Data collected are: Product ID (url used), Username, Location, Number of Reviews, Number of items sold, Last Active Date, Number of Likes, Price, Discount, Sizes Available, Brand\n",
    "        Item Condition, Colour, Style and Images of listing.\n",
    "        \"\"\"\n",
    "\n",
    "        data_dictionary = {}\n",
    "\n",
    "        data_dictionary.update({\"Product ID\": self.driver.current_url})\n",
    "        data_dictionary.update({\"UUID\": str(uuid.uuid4())})\n",
    "        \n",
    "        try:\n",
    "            shop_name = self.driver.find_element(by=By.CSS_SELECTOR, value=\"a[data-testid='bio__username']\").get_attribute(\"innerText\")\n",
    "            data_dictionary.update({\"Shop Name\": shop_name})\n",
    "        except:\n",
    "            data_dictionary.update({\"Shop Name\": \"ERROR\"})\n",
    "            print(\"Shop Name ERROR:\" + str(sys.exc_info()[0]))\n",
    "\n",
    "        try:\n",
    "            postcode = self.driver.find_element(by=By.CSS_SELECTOR, value=\"p[data-testid='bio__address']\").get_attribute(\"innerText\")\n",
    "            data_dictionary.update({\"Location\": postcode})\n",
    "        except:\n",
    "            data_dictionary.update({\"Location\": \"ERROR\"})\n",
    "            print(\"Location ERROR:\" + str(sys.exc_info()[0]))\n",
    "\n",
    "        try:\n",
    "            review_num = self.driver.find_element(by=By.CSS_SELECTOR, value=\"p[data-testid='feedback-btn__total']\").get_attribute(\"innerText\")\n",
    "            data_dictionary.update({\"No. of Reviews\": review_num})\n",
    "        except:\n",
    "            data_dictionary.update({\"No. of Reviews\": \"ERROR\"})\n",
    "            print(\"No. of Reviews ERROR:\" + str(sys.exc_info()[0]))\n",
    "        \n",
    "        try:\n",
    "            sold = self.driver.find_element(by=By.CSS_SELECTOR, value=\"div[data-testid='signals__sold']\")\n",
    "            items_sold = sold.find_element(by=By.XPATH, value=\".//p\").get_attribute(\"innerText\")\n",
    "            data_dictionary.update({\"No. of Items Sold\": items_sold})\n",
    "        except NoSuchElementException:\n",
    "            data_dictionary.update({\"No. of Items Sold\": \"0 sold\"})\n",
    "\n",
    "        try:\n",
    "            active = self.driver.find_element(by=By.CSS_SELECTOR, value=\"div[data-testid='signals__active']\")\n",
    "            last_activity = active.find_element(by=By.XPATH, value=\".//p\").get_attribute(\"innerText\")\n",
    "            data_dictionary.update({\"Last Active Date\": last_activity})\n",
    "        except:\n",
    "            data_dictionary.update({\"Last Active Date\": \"ERROR\"})\n",
    "            print(\"Last Active Date ERROR:\" + str(sys.exc_info()[0]))\n",
    "\n",
    "        try:\n",
    "            likes_num = self.driver.find_element(by=By.CSS_SELECTOR, value=\"span[data-testid='like-count']\").get_attribute(\"innerText\")\n",
    "        except NoSuchElementException:\n",
    "            likes_num = \"0\"\n",
    "        data_dictionary.update({\"No. of Likes\": likes_num})\n",
    "\n",
    "        try:\n",
    "            price = self.driver.find_element(by=By.CSS_SELECTOR, value=\"p[data-testid='discountedPrice']\").get_attribute(\"innerText\")\n",
    "            data_dictionary.update({\"Price\": price})\n",
    "            data_dictionary.update({\"Discount\": True})\n",
    "        except NoSuchElementException:\n",
    "            price = self.driver.find_element(by=By.CSS_SELECTOR, value=\"p[data-testid='fullPrice']\").get_attribute(\"innerText\")\n",
    "            data_dictionary.update({\"Price\": price})\n",
    "            data_dictionary.update({\"Discount\": True})\n",
    "\n",
    "        try:\n",
    "            item_description = self.driver.find_element(by=By.CSS_SELECTOR, value=\"p[data-testid='product__description']\").get_attribute(\"innerText\")\n",
    "        except:\n",
    "            item_description = \"ERROR\"\n",
    "            print(\"Item description ERROR:\" + str(sys.exc_info()[0]))\n",
    "        data_dictionary.update({\"Item Description\": item_description})\n",
    "\n",
    "        try:\n",
    "            last_refresh = self.driver.find_element(by=By.CSS_SELECTOR, value=\"time[data-testid='time']\").get_attribute(\"innerText\")\n",
    "        except:\n",
    "            last_refresh = \"ERROR\"\n",
    "            print(\"Last Refresh ERROR:\" + str(sys.exc_info()[0]))\n",
    "        data_dictionary.update({\"Last Update\": last_refresh})\n",
    "\n",
    "        try:\n",
    "            one_size = self.driver.find_element(by=By.CSS_SELECTOR, value=\"tr[data-testid='product__singleSize']\")\n",
    "            size = one_size.find_element(by=By.XPATH, value=\".//td\").get_attribute(\"innerText\")\n",
    "        except NoSuchElementException:\n",
    "            size = \"Multiple sizes\"\n",
    "        data_dictionary.update({\"Sizes Available\": size})\n",
    "\n",
    "        try:\n",
    "            brand = self.driver.find_element(by=By.CSS_SELECTOR, value=\"a[data-testid='product__brand']\").get_attribute(\"innerText\")\n",
    "        except NoSuchElementException:\n",
    "            brand = \"None\"\n",
    "        data_dictionary.update({\"Brand\": brand})\n",
    "\n",
    "        try:\n",
    "            condition = self.driver.find_element(by=By.CSS_SELECTOR, value=\"td[data-testid='product__condition']\").get_attribute(\"innerText\")\n",
    "        except:\n",
    "            condition = \"ERROR\"\n",
    "            print(\"Condition ERROR:\" + sys.exc_info()[0])\n",
    "        data_dictionary.update({\"Item Condition\": condition})\n",
    "\n",
    "        try:\n",
    "            colour = self.driver.find_element(by=By.CSS_SELECTOR, value=\"td[data-testid='product__colour']\").get_attribute(\"innerText\")\n",
    "        except NoSuchElementException:\n",
    "            colour = \"None\"\n",
    "        data_dictionary.update({\"Colour\": colour})\n",
    "\n",
    "        try:\n",
    "            style_tag = self.driver.find_element(by=By.CSS_SELECTOR, value=\"td[data-testid='selected__styles']\").get_attribute(\"innerText\")\n",
    "        except NoSuchElementException:\n",
    "            style_tag = \"None\"\n",
    "        data_dictionary.update({\"Style\": style_tag})\n",
    "\n",
    "        img_urls = []\n",
    "        try:\n",
    "            image_elements = self.driver.find_elements(by=By.CSS_SELECTOR, value=\"img[class='LazyLoadImage__StyledImage-sc-__bquzot-1 doaiRN styles__LazyImage-sc-__sc-1fk4zep-9 hRpLaq']\")\n",
    "            for image_element in image_elements:\n",
    "                img_url = image_element.get_attribute(\"src\")\n",
    "                if img_url not in img_urls:\n",
    "                    img_urls.append(img_url)\n",
    "            data_dictionary.update({\"Image Urls\": img_urls})\n",
    "\n",
    "            img_path = self.download_images(img_urls, self.driver.current_url, img_test_folder_name)\n",
    "            data_dictionary.update({\"Saved Images Path\": img_path})\n",
    "        except:\n",
    "            data_dictionary.update({\"Image Urls\": \"ERROR\"})\n",
    "            data_dictionary.update({\"Saved Images Path\": \"ERROR\"})\n",
    "            print(\"Image ERROR:\" + str(sys.exc_info()[0]))\n",
    "\n",
    "        return data_dictionary\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def scrape_listing(self, number_of_listing, dict_name):\n",
    "        \"\"\"\n",
    "        Scrapes data from listing on a specific webpage, eg.search webpage, store front webpage. Stores the scraped data in \n",
    "\n",
    "        Number of listign to scrape data from is defined by the input 'number_of _listing', 'dict_name' defines the dictionary name to stare the data under\n",
    "        \"\"\"\n",
    "\n",
    "        img_test_folder_name = \"./raw_data/images/\"\n",
    "        # os.mkdir(img_test_folder_name)\n",
    "\n",
    "        for i in range(number_of_listing):\n",
    "            if ((i)%24 == 0) and i !=0:\n",
    "                self.scroll_to_bottom()\n",
    "                time.sleep(1)\n",
    "            listing_url = self.listing_url(i)\n",
    "            self.open_url_new_tab(listing_url)\n",
    "            self.switch_tab(1)\n",
    "            self.scroll_to_bottom()\n",
    "            scraped_data = self.get_product_page_data(img_test_folder_name)\n",
    "            self.add_data(scraped_data, \"./raw_data/data.json\", dict_name)\n",
    "            self.close_tab()\n",
    "            self.switch_tab(0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def create_json_file(self, filepath, main_dictionaries,):\n",
    "        \"\"\"\n",
    "        Method to create a JSON file\n",
    "        \"\"\"\n",
    "\n",
    "        output ={}\n",
    "        for dictionary in main_dictionaries:\n",
    "            output[dictionary]=[]\n",
    "        with open(filepath, \"w\") as outfile:\n",
    "            json.dump(output, outfile)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def add_data(self, new_data, filepath, main_dictionary):\n",
    "        \"\"\"\n",
    "        Method to add data to the JSON file\n",
    "        \"\"\"\n",
    "\n",
    "        with open(filepath, 'r+') as file:\n",
    "            file_data = json.load(file)\n",
    "            file_data[main_dictionary].append(new_data)\n",
    "            file.seek(0)\n",
    "            json.dump(file_data, file)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def download_images(self, url_list, product_id, img_test_folder_name):\n",
    "        \"\"\"\n",
    "        Method to download images with from a url\n",
    "        \"\"\"\n",
    "\n",
    "        folder = product_id[22:]\n",
    "        folder_name = folder.replace('/', '-')\n",
    "        copy = False\n",
    "        copy_num = \"\"\n",
    "        \n",
    "        img_path = \"\"\n",
    "        try:\n",
    "            img_folder_name = img_test_folder_name+\"/\"+folder_name\n",
    "            os.mkdir(img_folder_name)\n",
    "        except FileExistsError:\n",
    "            copy_num = str(uuid.uuid4())\n",
    "            img_folder_name = img_test_folder_name+\"/\"+folder_name+copy_num\n",
    "            os.mkdir(img_folder_name)\n",
    "            copy = True\n",
    "        for i in range(len(url_list)):\n",
    "            if copy == False:    \n",
    "                img_path = img_test_folder_name+\"/\"+folder_name\n",
    "                open(img_path+\"/\"+str(i)+\".jpg\", 'w').close()\n",
    "                urllib.request.urlretrieve(url_list[i], img_path+\"/\"+str(i)+\".jpg\")\n",
    "            else:\n",
    "                img_path = img_test_folder_name+\"/\"+folder_name+copy_num\n",
    "                open(img_path+\"/\"+str(i)+\".jpg\", 'w').close()\n",
    "                urllib.request.urlretrieve(url_list[i], img_path+\"/\"+str(i)+\".jpg\")\n",
    "\n",
    "        return img_path\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "    # Scrapper()\n",
    "    # Scrapper.nav_by_search(Scrapper, \"top\")\n",
    "    # Scrapper.nav_by_url(Scrapper, Scrapper.header_url_list(Scrapper)[1])\n",
    "    # Scrapper.scroll_to_bottom(Scrapper)\n",
    "    # Scrapper.back_page(Scrapper)\n",
    "    # Scrapper.open_url_new_tab(Scrapper, Scrapper.nav_listing(Scrapper, 2))\n",
    "    # Scrapper.switch_tab(Scrapper, 0)\n",
    "    # Scrapper.close_tab(Scrapper)\n",
    "    # bot = Scrapper()\n",
    "    # print(bot.header_url_list())\n",
    "    # bot.nav_by_shop(\"cleopatress\")\n",
    "    # print(\"Current Page Title is : %s\" %bot.driver.title)\n",
    "    # bot.create_json_file(\"./raw_data/data.json\", [\"Test_ShopData\"])\n",
    "    # bot.scrape_listing(150, \"Test_ShopData\")\n",
    "    # bot.close_browser()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "\n",
    "class ScrapperTestCase(unittest.TestCase):\n",
    "\n",
    "    @classmethod\n",
    "    def setUpClass(cls):\n",
    "        cls.driver = webdriver.Chrome(service = Service('./chromedriver'))\n",
    "        cls.driver.get(\"https://www.depop.com\")\n",
    "        \n",
    "\n",
    "    @classmethod\n",
    "    def tearDownClass(cls):\n",
    "        cls.driver.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Product ID': 'https://www.depop.com/products/chlloeharrison-topshop-washed-out-greyblack-mom/', 'UUID': '2488808d-4a4e-41ba-965e-7aaf51b4b808', 'Shop Name': 'chlloeharrison', 'Location': 'Washington, United Kingdom', 'No. of Reviews': '579', 'No. of Items Sold': '602 sold', 'Last Active Date': 'Active today', 'No. of Likes': '2 likes', 'Price': 'Â£21.75', 'Discount': True, 'Item Description': 'Topshop washed out grey/black mom jeans. Worn once. Petite W25 L28.\\n#topshop #momjeans', 'Last Update': 'LISTED 46 MINUTES AGO', 'Sizes Available': 'Multiple sizes', 'Brand': 'Topshop', 'Item Condition': 'Like new', 'Colour': 'Grey, Black', 'Style': 'Streetwear, Loungewear, Minimalist, Modern', 'Image Urls': ['https://media-photos.depop.com/b0/7486950/1213226473_5cc7c666328d4c36b13213604d821f5e/P0.jpg'], 'Saved Images Path': './raw_data/images//products-chlloeharrison-topshop-washed-out-greyblack-mom-b4c6e959-9978-489c-948d-7cb93da42ce9'}\n",
      "19\n"
     ]
    }
   ],
   "source": [
    "bot = Scrapper()\n",
    "bot.driver.get(\"https://www.depop.com/products/chlloeharrison-topshop-washed-out-greyblack-mom/\")\n",
    "hi=bot.get_product_page_data('./raw_data/images/')\n",
    "print(hi)\n",
    "print(len(hi))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hell\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict = {\"hi\":\"hello\", 'bye': 'goodbye'}\n",
    "print(dict[\"hi\"][:4])\n",
    "len(\"https://media-photos.depop.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './raw_data/test'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/serenawong/Desktop/data-collection-pipeline/scrapper_class.ipynb Cell 6'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/serenawong/Desktop/data-collection-pipeline/scrapper_class.ipynb#ch0000005?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mshutil\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/serenawong/Desktop/data-collection-pipeline/scrapper_class.ipynb#ch0000005?line=1'>2</a>\u001b[0m shutil\u001b[39m.\u001b[39;49mrmtree(\u001b[39m'\u001b[39;49m\u001b[39m./raw_data/test\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m~/miniforge3/envs/data-collection-pipeline-env/lib/python3.9/shutil.py:722\u001b[0m, in \u001b[0;36mrmtree\u001b[0;34m(path, ignore_errors, onerror)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/serenawong/miniforge3/envs/data-collection-pipeline-env/lib/python3.9/shutil.py?line=719'>720</a>\u001b[0m     orig_st \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mlstat(path)\n\u001b[1;32m    <a href='file:///Users/serenawong/miniforge3/envs/data-collection-pipeline-env/lib/python3.9/shutil.py?line=720'>721</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///Users/serenawong/miniforge3/envs/data-collection-pipeline-env/lib/python3.9/shutil.py?line=721'>722</a>\u001b[0m     onerror(os\u001b[39m.\u001b[39;49mlstat, path, sys\u001b[39m.\u001b[39;49mexc_info())\n\u001b[1;32m    <a href='file:///Users/serenawong/miniforge3/envs/data-collection-pipeline-env/lib/python3.9/shutil.py?line=722'>723</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/serenawong/miniforge3/envs/data-collection-pipeline-env/lib/python3.9/shutil.py?line=723'>724</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniforge3/envs/data-collection-pipeline-env/lib/python3.9/shutil.py:720\u001b[0m, in \u001b[0;36mrmtree\u001b[0;34m(path, ignore_errors, onerror)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/serenawong/miniforge3/envs/data-collection-pipeline-env/lib/python3.9/shutil.py?line=716'>717</a>\u001b[0m \u001b[39m# Note: To guard against symlink races, we use the standard\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/serenawong/miniforge3/envs/data-collection-pipeline-env/lib/python3.9/shutil.py?line=717'>718</a>\u001b[0m \u001b[39m# lstat()/open()/fstat() trick.\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/serenawong/miniforge3/envs/data-collection-pipeline-env/lib/python3.9/shutil.py?line=718'>719</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///Users/serenawong/miniforge3/envs/data-collection-pipeline-env/lib/python3.9/shutil.py?line=719'>720</a>\u001b[0m     orig_st \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39;49mlstat(path)\n\u001b[1;32m    <a href='file:///Users/serenawong/miniforge3/envs/data-collection-pipeline-env/lib/python3.9/shutil.py?line=720'>721</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[1;32m    <a href='file:///Users/serenawong/miniforge3/envs/data-collection-pipeline-env/lib/python3.9/shutil.py?line=721'>722</a>\u001b[0m     onerror(os\u001b[39m.\u001b[39mlstat, path, sys\u001b[39m.\u001b[39mexc_info())\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './raw_data/test'"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "shutil.rmtree('./raw_data/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 3):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "8\n",
      "6\n",
      "4\n",
      "2\n",
      "0\n",
      "-2\n",
      "-4\n"
     ]
    }
   ],
   "source": [
    "for i in range(10, -6, -2):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "59a815aa6b510b7c00ae6aa3124b3da72856d47b254de79486eb33c0d94934ee"
  },
  "kernelspec": {
   "display_name": "Python 3.9.11 ('data-collection-pipeline-env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
