{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "import time\n",
    "import uuid\n",
    "import json\n",
    "import os\n",
    "import unittest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no (accept cookies button) found\n",
      "Current Page Title is : Cleopatress 's Shop - Depop\n"
     ]
    },
    {
     "ename": "FileExistsError",
     "evalue": "[Errno 17] File exists: './raw_data/images/products-suzy_a-white-halter-neck-ruched-mesh-'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m/Users/serenawong/Desktop/data-collection-pipeline/scrapper_class.ipynb Cell 2'\u001b[0m in \u001b[0;36m<cell line: 320>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/serenawong/Desktop/data-collection-pipeline/scrapper_class.ipynb#ch0000001?line=317'>318</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mCurrent Page Title is : \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39mdriver\u001b[39m.\u001b[39mtitle)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/serenawong/Desktop/data-collection-pipeline/scrapper_class.ipynb#ch0000001?line=318'>319</a>\u001b[0m bot\u001b[39m.\u001b[39mcreate_json_file(\u001b[39m\"\u001b[39m\u001b[39m./raw_data/data.json\u001b[39m\u001b[39m\"\u001b[39m, [\u001b[39m\"\u001b[39m\u001b[39mTest_ShopData\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/serenawong/Desktop/data-collection-pipeline/scrapper_class.ipynb#ch0000001?line=319'>320</a>\u001b[0m bot\u001b[39m.\u001b[39;49mscrape_listing(\u001b[39m150\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mTest_ShopData\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "\u001b[1;32m/Users/serenawong/Desktop/data-collection-pipeline/scrapper_class.ipynb Cell 2'\u001b[0m in \u001b[0;36mScrapper.scrape_listing\u001b[0;34m(self, number_of_listing, dict_name)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/serenawong/Desktop/data-collection-pipeline/scrapper_class.ipynb#ch0000001?line=258'>259</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mswitch_tab(\u001b[39m1\u001b[39m)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/serenawong/Desktop/data-collection-pipeline/scrapper_class.ipynb#ch0000001?line=259'>260</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscroll_to_bottom()\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/serenawong/Desktop/data-collection-pipeline/scrapper_class.ipynb#ch0000001?line=260'>261</a>\u001b[0m scraped_data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_product_page_data()\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/serenawong/Desktop/data-collection-pipeline/scrapper_class.ipynb#ch0000001?line=261'>262</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd_data(scraped_data, \u001b[39m\"\u001b[39m\u001b[39m./raw_data/data.json\u001b[39m\u001b[39m\"\u001b[39m, dict_name)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/serenawong/Desktop/data-collection-pipeline/scrapper_class.ipynb#ch0000001?line=262'>263</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose_tab()\n",
      "\u001b[1;32m/Users/serenawong/Desktop/data-collection-pipeline/scrapper_class.ipynb Cell 2'\u001b[0m in \u001b[0;36mScrapper.get_product_page_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/serenawong/Desktop/data-collection-pipeline/scrapper_class.ipynb#ch0000001?line=237'>238</a>\u001b[0m         img_urls\u001b[39m.\u001b[39mappend(img_url)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/serenawong/Desktop/data-collection-pipeline/scrapper_class.ipynb#ch0000001?line=238'>239</a>\u001b[0m data_dictionary\u001b[39m.\u001b[39mupdate({\u001b[39m\"\u001b[39m\u001b[39mImage Urls\u001b[39m\u001b[39m\"\u001b[39m: img_urls})\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/serenawong/Desktop/data-collection-pipeline/scrapper_class.ipynb#ch0000001?line=240'>241</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdownload_images(img_urls, driver\u001b[39m.\u001b[39;49mcurrent_url)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/serenawong/Desktop/data-collection-pipeline/scrapper_class.ipynb#ch0000001?line=242'>243</a>\u001b[0m \u001b[39m# print(rating)\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/serenawong/Desktop/data-collection-pipeline/scrapper_class.ipynb#ch0000001?line=244'>245</a>\u001b[0m \u001b[39mreturn\u001b[39;00m data_dictionary\n",
      "\u001b[1;32m/Users/serenawong/Desktop/data-collection-pipeline/scrapper_class.ipynb Cell 2'\u001b[0m in \u001b[0;36mScrapper.download_images\u001b[0;34m(self, url_list, product_id)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/serenawong/Desktop/data-collection-pipeline/scrapper_class.ipynb#ch0000001?line=289'>290</a>\u001b[0m folder \u001b[39m=\u001b[39m product_id[\u001b[39m22\u001b[39m:]\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/serenawong/Desktop/data-collection-pipeline/scrapper_class.ipynb#ch0000001?line=290'>291</a>\u001b[0m folder_name \u001b[39m=\u001b[39m folder\u001b[39m.\u001b[39mreplace(\u001b[39m'\u001b[39m\u001b[39m/\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m-\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/serenawong/Desktop/data-collection-pipeline/scrapper_class.ipynb#ch0000001?line=291'>292</a>\u001b[0m os\u001b[39m.\u001b[39;49mmkdir(\u001b[39m\"\u001b[39;49m\u001b[39m./raw_data/images/\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m+\u001b[39;49mfolder_name)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/serenawong/Desktop/data-collection-pipeline/scrapper_class.ipynb#ch0000001?line=292'>293</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(url_list)):\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/serenawong/Desktop/data-collection-pipeline/scrapper_class.ipynb#ch0000001?line=293'>294</a>\u001b[0m     \u001b[39mopen\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m./raw_data/images/\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m+\u001b[39mfolder_name\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m+\u001b[39m\u001b[39mstr\u001b[39m(i)\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.jpg\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mclose()\n",
      "\u001b[0;31mFileExistsError\u001b[0m: [Errno 17] File exists: './raw_data/images/products-suzy_a-white-halter-neck-ruched-mesh-'"
     ]
    }
   ],
   "source": [
    "driver = webdriver.Chrome(service = Service('./chromedriver'))\n",
    "driver.get(\"https://www.depop.com\")\n",
    "    \n",
    "class Scrapper:\n",
    "\n",
    "    \"\"\"\n",
    "    This class contains methods to scrape data from the website 'www.depop.com'.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "\n",
    "        \"\"\"\n",
    "        Checks if 'accept cookies' button is present, and by passes it if present.\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            driver.find_element(by=By.CSS_SELECTOR, value=\"button[class='sc-kEqXSa sc-iqAclL sc-ciSkZP hQtFsL cmWQHQ exduyW']\").click()\n",
    "        except NoSuchElementException:\n",
    "            print(\"no (accept cookies button) found\")\n",
    "\n",
    "\n",
    "    def nav_by_search(self, search_item):\n",
    "\n",
    "        \"\"\"\n",
    "        Navigate to a specific search result corresponding to the 'search_item' input string.\n",
    "        \"\"\"\n",
    "\n",
    "        search_url = \"https://www.depop.com/search/?q=\"+search_item\n",
    "        driver.get(search_url)\n",
    "\n",
    "    def nav_by_shop(self, shop_name):\n",
    "\n",
    "        \"\"\"\n",
    "        Navigate to a specific store front corresponding to the 'shop_name' input string.\n",
    "        \"\"\"\n",
    "\n",
    "        shop_url = \"https://www.depop.com/\"+shop_name\n",
    "        driver.get(shop_url)\n",
    "\n",
    "    def header_url_list(self):\n",
    "\n",
    "        \"\"\"\n",
    "        Returns a list with the URL from the webpage header.\n",
    "        \"\"\"\n",
    "\n",
    "        header_url = []\n",
    "        top_level_elements = driver.find_elements(by=By.CLASS_NAME, value=\"styles__NavigationItem-sc-__sc-10mkzda-3\")\n",
    "        for i in top_level_elements:\n",
    "            try:\n",
    "                child_level_elements = i.find_elements(by=By.XPATH, value=\".//div/ul/li\")\n",
    "                for j in child_level_elements:\n",
    "                    child_nav_option = j.find_element(by=By.XPATH, value=\".//a\").get_attribute(\"href\")\n",
    "                    header_url.append(child_nav_option)\n",
    "            except NoSuchElementException:\n",
    "                pass\n",
    "        return header_url\n",
    "\n",
    "\n",
    "    def listing_url(self, n):\n",
    "\n",
    "        \"\"\"\n",
    "        Takes the input 'n' and returns the URL of the nth listing.\n",
    "        \"\"\"\n",
    "        listing = driver.find_elements(by=By.CLASS_NAME, value=\"styles__ProductCardContainer-sc-__sc-13q41bc-8\")\n",
    "        listing_url = listing[n].find_element(by=By.XPATH, value=\".//a\").get_attribute(\"href\")\n",
    "        return listing_url\n",
    "\n",
    "    def scroll_to_bottom(self):\n",
    "\n",
    "        \"\"\"\n",
    "        Scrolls to the bottom of webpage.\n",
    "        \"\"\"\n",
    "\n",
    "        webpage_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        current_height = 0\n",
    "        while current_height <= webpage_height:\n",
    "            driver.execute_script(\"window.scrollTo(0,\"+str(current_height)+\")\")\n",
    "            current_height += 30\n",
    "\n",
    "    def back_page(self):\n",
    "\n",
    "        \"\"\"\n",
    "        Navigates to the previous webpage.\n",
    "        \"\"\"\n",
    "\n",
    "        driver.execute_script(\"window.history.go(-1)\")\n",
    "\n",
    "    def open_url_new_tab(self, url):\n",
    "        \"\"\"\n",
    "        Takes an input URL string 'url' and opens the URL in a new tab.\n",
    "        \"\"\"\n",
    "        driver.execute_script(\"window.open('\" + url + \"');\")\n",
    "\n",
    "    def close_tab(self):\n",
    "        \"\"\"\n",
    "        Closes current active tab.\n",
    "        \"\"\"\n",
    "        driver.close()\n",
    "\n",
    "    def switch_tab(self, tab_no):\n",
    "        \"\"\"\n",
    "        Switch to tab with the index 'tab_no'.\n",
    "        \"\"\"\n",
    "        driver.switch_to.window(driver.window_handles[tab_no])\n",
    "\n",
    "    def get_shop_data(self):\n",
    "        \"\"\"\n",
    "        Returns data from a store front in the form of a dictionary.\n",
    "        \n",
    "        Data collected are: Username, Items sold, Last Active Date, Followers, Following, Bio description.\n",
    "        \"\"\"\n",
    "        data_dictionary={}\n",
    "\n",
    "        username = driver.find_element(by=By.CLASS_NAME, value=\"styles__UserName-sc-__r941b9-4\").get_attribute(\"innerText\")\n",
    "        data_dictionary.update({\"Username\": username})\n",
    "        items_sold = driver.find_element(by=By.XPATH, value=\"//*[@id='main']/div[1]/div[1]/div/div[2]/div[1]/p\").get_attribute(\"innerText\")\n",
    "        data_dictionary.update({\"Items Sold\": items_sold})\n",
    "        last_activity = driver.find_element(by=By.XPATH, value='//*[@id=\"main\"]/div[1]/div[1]/div/div[2]/div[2]/p').get_attribute(\"innerText\")\n",
    "        data_dictionary.update({\"Last Active Date\": last_activity})\n",
    "        followers = driver.find_element(by=By.XPATH, value='//*[@id=\"main\"]/div[1]/div[2]/button[1]/p[1]').get_attribute(\"innerText\")\n",
    "        data_dictionary.update({\"Followers\": followers})\n",
    "        following = driver.find_element(by=By.XPATH, value='//*[@id=\"main\"]/div[1]/div[2]/button[2]/p[1]').get_attribute(\"innerText\")\n",
    "        data_dictionary.update({\"Following\": following})\n",
    "        try:\n",
    "            bio_text = driver.find_element(by=By.XPATH, value='//*[@id=\"main\"]/div[1]/div[3]/p').get_attribute(\"innerText\")\n",
    "        except NoSuchElementException:\n",
    "            bio_text = \"None\"\n",
    "        data_dictionary.update({\"Bio Description\":bio_text})\n",
    "        return data_dictionary        \n",
    "\n",
    "    def product_availability(self):\n",
    "        \"\"\"\n",
    "        Checks if an item is sold.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            driver.find_element(by=By.CSS_SELECTOR, value=\"button.egHolT[color='yellow']\")\n",
    "            sold = True\n",
    "        except NoSuchElementException:\n",
    "            sold = False\n",
    "        print(\"Product sold? \"+ str(sold))\n",
    "        return sold\n",
    "\n",
    "\n",
    "    def get_product_page_data(self):\n",
    "        \"\"\"\n",
    "        Returns data from a item listing in the form of a dictionary, and generates a UUID to identify data point.\n",
    "        \n",
    "        Data collected are: Product ID (url used), Username, Location, Number of Reviews, Number of items sold, Last Active Date, Number of Likes, Price, Discount, Sizes Available, Brand\n",
    "        Item Condition, Colour, Style and Images of listing.\n",
    "        \"\"\"\n",
    "\n",
    "        data_dictionary = {}\n",
    "\n",
    "        data_dictionary.update({\"Product ID\": driver.current_url})\n",
    "        data_dictionary.update({\"UUID\": str(uuid.uuid4())})\n",
    "        shop_name = driver.find_element(by=By.CSS_SELECTOR, value=\"a[data-testid='bio__username']\").get_attribute(\"innerText\")\n",
    "        data_dictionary.update({\"Shop Name\": shop_name})\n",
    "        postcode = driver.find_element(by=By.CSS_SELECTOR, value=\"p[data-testid='bio__address']\").get_attribute(\"innerText\")\n",
    "        data_dictionary.update({\"Location\": postcode})\n",
    "\n",
    "        # <p data-testid=\"bio__address\" type=\"caption1\" class=\"sc-jrsJWt eoZhdh\">United Kingdom</p>\n",
    "\n",
    "        # rating = 0\n",
    "        # for i in range(1, 5):\n",
    "\n",
    "        #     star = driver.find_element(by=By.XPATH, value=\"//*[@id='feedback-star-\" + str(i) + \"-19262048']/title\").get_attribute(\"innerText\")\n",
    "        #     if star == \"Full Star\":\n",
    "        #         rating += 1\n",
    "        #     elif star == \"Half Star\":\n",
    "        #         rating += 0.5\n",
    "        #     elif star == \"Empty Star\":\n",
    "        #         rating += 0\n",
    "        \n",
    "        review_num = driver.find_element(by=By.CSS_SELECTOR, value=\"p[data-testid='feedback-btn__total']\").get_attribute(\"innerText\")\n",
    "        data_dictionary.update({\"No. of Reviews\": review_num})\n",
    "        sold = driver.find_element(by=By.CSS_SELECTOR, value=\"div[data-testid='signals__sold']\")\n",
    "        items_sold = sold.find_element(by=By.XPATH, value=\".//p\").get_attribute(\"innerText\")\n",
    "\n",
    "        data_dictionary.update({\"No. of Items Sold\": items_sold})\n",
    "        active = driver.find_element(by=By.CSS_SELECTOR, value=\"div[data-testid='signals__active']\")\n",
    "        last_activity = active.find_element(by=By.XPATH, value=\".//p\").get_attribute(\"innerText\")\n",
    "        data_dictionary.update({\"Last Active Date\": last_activity})\n",
    "\n",
    "        try:\n",
    "            likes_num = driver.find_element(by=By.CSS_SELECTOR, value=\"span[data-testid='like-count']\").get_attribute(\"innerText\")\n",
    "        except NoSuchElementException:\n",
    "            likes_num = \"0\"\n",
    "        data_dictionary.update({\"No. of Likes\": likes_num})\n",
    "\n",
    "        try:\n",
    "            price = driver.find_element(by=By.CSS_SELECTOR, value=\"p[data-testid='discountedPrice']\").get_attribute(\"innerText\")\n",
    "            data_dictionary.update({\"Price\": price})\n",
    "            data_dictionary.update({\"Discount\": True})\n",
    "        except NoSuchElementException:\n",
    "            price = driver.find_element(by=By.CSS_SELECTOR, value=\"p[data-testid='fullPrice']\").get_attribute(\"innerText\")\n",
    "            data_dictionary.update({\"Price\": price})\n",
    "            data_dictionary.update({\"Discount\": True})\n",
    "\n",
    "        item_description = driver.find_element(by=By.CSS_SELECTOR, value=\"p[data-testid='product__description']\").get_attribute(\"innerText\")\n",
    "        data_dictionary.update({\"Item Description\": item_description})\n",
    "        last_refresh = driver.find_element(by=By.CSS_SELECTOR, value=\"time[data-testid='time']\").get_attribute(\"innerText\")\n",
    "        data_dictionary.update({\"Last Update\": last_refresh})\n",
    "\n",
    "        try:\n",
    "            one_size = driver.find_element(by=By.CSS_SELECTOR, value=\"tr[data-testid='product__singleSize']\")\n",
    "            size = one_size.find_element(by=By.XPATH, value=\".//td\").get_attribute(\"innerText\")\n",
    "\n",
    "        except NoSuchElementException:\n",
    "            size = \"Multiple sizes\"\n",
    "        data_dictionary.update({\"Sizes Available\": size})\n",
    "\n",
    "        try:\n",
    "            brand = driver.find_element(by=By.CSS_SELECTOR, value=\"a[data-testid='product__brand']\").get_attribute(\"innerText\")\n",
    "        except NoSuchElementException:\n",
    "            brand = \"None\"\n",
    "        data_dictionary.update({\"Brand\": brand})\n",
    "\n",
    "        condition = driver.find_element(by=By.CSS_SELECTOR, value=\"td[data-testid='product__condition']\").get_attribute(\"innerText\")\n",
    "        data_dictionary.update({\"Item Condition\": condition})\n",
    "\n",
    "        try:\n",
    "            colour = driver.find_element(by=By.CSS_SELECTOR, value=\"td[data-testid='product__colour']\").get_attribute(\"innerText\")\n",
    "        except NoSuchElementException:\n",
    "            colour = \"None\"\n",
    "        data_dictionary.update({\"Colour\": colour})\n",
    "\n",
    "        try:\n",
    "            style_tag = driver.find_element(by=By.CSS_SELECTOR, value=\"td[data-testid='selected__styles']\").get_attribute(\"innerText\")\n",
    "        except NoSuchElementException:\n",
    "            style_tag = \"None\"\n",
    "        data_dictionary.update({\"Style\": style_tag})\n",
    "\n",
    "        img_urls = []\n",
    "        image_elements = driver.find_elements(by=By.CSS_SELECTOR, value=\"img[class='LazyLoadImage__StyledImage-sc-__bquzot-1 doaiRN styles__LazyImage-sc-__sc-1fk4zep-9 hRpLaq']\")\n",
    "        for image_element in image_elements:\n",
    "            img_url = image_element.get_attribute(\"src\")\n",
    "            if img_url not in img_urls:\n",
    "                img_urls.append(img_url)\n",
    "        data_dictionary.update({\"Image Urls\": img_urls})\n",
    "\n",
    "        self.download_images(img_urls, driver.current_url)\n",
    "\n",
    "        # print(rating)\n",
    "\n",
    "        return data_dictionary\n",
    "\n",
    "    def scrape_listing(self, number_of_listing, dict_name):\n",
    "        \"\"\"\n",
    "        Scrapes data from listing on a specific webpage, eg.search webpage, store front webpage. Stores the scraped data in \n",
    "\n",
    "        Number of listign to scrape data from is defined by the input 'number_of _listing', 'dict_name' defines the dictionary name to stare the data under\n",
    "        \"\"\"\n",
    "        for i in range(number_of_listing):\n",
    "            if ((i)%24 == 0) and i !=0:\n",
    "                self.scroll_to_bottom()\n",
    "                time.sleep(1)\n",
    "            listing_url = self.listing_url(i)\n",
    "            self.open_url_new_tab(listing_url)\n",
    "            self.switch_tab(1)\n",
    "            self.scroll_to_bottom()\n",
    "            scraped_data = self.get_product_page_data()\n",
    "            self.add_data(scraped_data, \"./raw_data/data.json\", dict_name)\n",
    "            self.close_tab()\n",
    "            self.switch_tab(0)\n",
    "\n",
    "    def create_json_file(self, filepath, main_dictionaries,):\n",
    "        \"\"\"\n",
    "        Method to create a JSON file\n",
    "        \"\"\"\n",
    "        output ={}\n",
    "        for dictionary in main_dictionaries:\n",
    "            output[dictionary]=[]\n",
    "        with open(filepath, \"w\") as outfile:\n",
    "            json.dump(output, outfile)\n",
    "\n",
    "    def add_data(self, new_data, filepath, main_dictionary):\n",
    "        \"\"\"\n",
    "        Method to add data to the JSON file\n",
    "        \"\"\"\n",
    "        with open(filepath, 'r+') as file:\n",
    "            file_data = json.load(file)\n",
    "            file_data[main_dictionary].append(new_data)\n",
    "            file.seek(0)\n",
    "            json.dump(file_data, file)\n",
    "\n",
    "    def download_images(self, url_list, product_id):\n",
    "        \"\"\"\n",
    "        Method to download images with from a url\n",
    "        \"\"\"\n",
    "        folder = product_id[22:]\n",
    "        folder_name = folder.replace('/', '-')\n",
    "        os.mkdir(\"./raw_data/images/\"+folder_name)\n",
    "        for i in range(len(url_list)):\n",
    "            open(\"./raw_data/images/\"+folder_name+\"/\"+str(i)+\".jpg\", 'w').close()\n",
    "            urllib.request.urlretrieve(url_list[i], \"./raw_data/images/\"+folder_name+\"/\"+str(i)+\".jpg\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    Scrapper()\n",
    "    # Scrapper.nav_by_search(Scrapper, \"top\")\n",
    "    # Scrapper.nav_by_url(Scrapper, Scrapper.header_url_list(Scrapper)[1])\n",
    "    # Scrapper.scroll_to_bottom(Scrapper)\n",
    "    # Scrapper.back_page(Scrapper)\n",
    "    # Scrapper.open_url_new_tab(Scrapper, Scrapper.nav_listing(Scrapper, 2))\n",
    "    # Scrapper.switch_tab(Scrapper, 0)\n",
    "    # Scrapper.close_tab(Scrapper)\n",
    "\n",
    "bot = Scrapper()\n",
    "bot.nav_by_shop(\"cleopatress\")\n",
    "print(\"Current Page Title is : %s\" %driver.title)\n",
    "bot.create_json_file(\"./raw_data/data.json\", [\"Test_ShopData\"])\n",
    "bot.scrape_listing(150, \"Test_ShopData\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "\n",
    "list = [\"https://media-photos.depop.com/b0/14667927/792417311_9d826f5f4382490cb28fdb1bb13a3db1/P0.jpg\", \"https://media-photos.depop.com/b0/14667927/792417332_6df7c572bc46451ba5adf066fc959b37/P0.jpg\"]\n",
    "for i in range(len(list)):\n",
    "    open(\"./raw_data/images/\"+\"test/\"+str(i)+\".jpg\", 'w').close()\n",
    "# fullfilename = os.path.join(\"images/test\", \"1.jpg\")\n",
    "    urllib.request.urlretrieve(list[i], \"./raw_data/images/\"+\"test/\"+str(i)+\".jpg\")\n",
    "\n",
    "# with open(\"./raw_data/images/\"+\"test\", )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir(\"raw_data/images/products/robinrebecca-y2k-top-white-open-front-7494/\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'unittest' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/serenawong/Desktop/data-collection-pipeline/scrapper_class.ipynb Cell 5'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/serenawong/Desktop/data-collection-pipeline/scrapper_class.ipynb#ch0000003?line=0'>1</a>\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mScrapperTestCase\u001b[39;00m(unittest\u001b[39m.\u001b[39mTestCase):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/serenawong/Desktop/data-collection-pipeline/scrapper_class.ipynb#ch0000003?line=1'>2</a>\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/serenawong/Desktop/data-collection-pipeline/scrapper_class.ipynb#ch0000003?line=3'>4</a>\u001b[0m unittest\u001b[39m.\u001b[39mmain()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'unittest' is not defined"
     ]
    }
   ],
   "source": [
    "class ScrapperTestCase(unittest.TestCase):\n",
    "    pass\n",
    "\n",
    "unittest.main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "59a815aa6b510b7c00ae6aa3124b3da72856d47b254de79486eb33c0d94934ee"
  },
  "kernelspec": {
   "display_name": "Python 3.9.11 ('data-collection-pipeline-env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
